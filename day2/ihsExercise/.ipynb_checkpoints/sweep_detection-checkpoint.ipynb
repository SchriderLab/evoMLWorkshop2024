{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a0e9bde",
   "metadata": {},
   "source": [
    "# Detecting selective sweeps\n",
    "\n",
    "In this exercise we are going to get our hands dirty lookinag at population genetic signatures of adaptation. When a beneficial mutation occurs, it may rapidly spread through the population, resulting in a selective sweep. We talked a bit about this process in lecture, and how it impacts patterns of genetic diversity in the area around the selected mutation. In this exercise, we will learn how to use a statistic called _iHS_ to try detect recent selective sweeps. We will start with an overview of the method, then apply it to simulated data to see how it works, and then finally take it out for a spin on some real genomic data from a human population.\n",
    "\n",
    "## Part 0: Downloading a couple of files.\n",
    "\n",
    "First we have to download a couple of files into your working directory. One file has genomic data for the final part of the exercise, and the other is just a figure that will help us understand the _iHS_ statistic. Run these commands to grab them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e0a53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget 'https://raw.githubusercontent.com/SchriderLab/selectionScanExercise/main/misc/ihs_fig.png'\n",
    "!wget 'https://raw.githubusercontent.com/SchriderLab/selectionScanExercise/main/preCookedData/CEU50.chr2LCT.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d952b",
   "metadata": {},
   "source": [
    "## Part 1: Understanding the integrated haplotype score (_iHS_)\n",
    "\n",
    "In the lecture, we briefly talked about a few statistics used to summarize patterns of genetic variation. The _iHS_ statistic is a bit more complex than those in its formulation, but our goal here will be just to understand it conceptually. Let's start by taking a look at a figure from Voight et al. (2006), the paper that introduced _iHS_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67a9a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have to import the Image function to view an image file in this notebook\n",
    "from IPython.display import Image\n",
    "\n",
    "# Now we can display the figure, which should be present in your working directory.\n",
    "Image('ihs_fig.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee05caf",
   "metadata": {},
   "source": [
    "At first this looks like some bad abstract art, but we can make sense of it. It is really just a representation of a population genetic alignment: the _x_-axis shows different SNPs, and each haplotype is shown as a separate row on the _y_-axis. But instead of looking at \"A\"s, \"C\"s, \"G\"s, and \"T\"s, we are representing genotypes by colors. The color scheme will make sense if we start at the center: the Test SNP. This is the SNP that we are testing for evidence of positive selection. Has it rapidly increased in frequency (i.e. is it involved in an ongoing selective sweep) or not? At the Test SNP, we have two alleles, the ancestral allele and the derived allele, which we will color as blue and red, respectively.\n",
    "\n",
    "First, let's focus on individuals with the ancestral allele (they are all colored blue at the Test SNP, and present in the top half of this figure), and let's move out to the right, looking at sites in the genome to the right of our Test SNP. At first, we see a column that is all blue--that is, all individuals with the ancestral/blue allele are our Test SNP are also identical to one another at these other sites to the right. However, we pretty quickly come to a point where some of our individuals are green. This is because we have reached a point where our individuals having the ancestral/blue allele at the Test SNP are no longer identical: at this site, there is a SNP where some of these individuals have the \"green\" allele, so we color them green, while most do not, and thus they remain blue. Note that the coloring here is arbitrary, green and blue could easily have been swapped with purple and orange or whatever, but the coloring has been done to simply highlight differences among individuals harboring the ancestral allele at our Test SNP. We press on, further to the right, and quickly find some more new colors popping up, representing more differences among individuals with the ancestral allele at the Test SNP. We see that as we move further and further away, we find more differences. This is because recombination and mutation events break up similarities among individuals, and the wider the genomic region you examine, the more opportunities for these events there have been.\n",
    "\n",
    "Now, let's switch our focus to individuals with the derived allele (red). We again see that these individuals are all identical at the Test SNP, which must be the case by definition, and as we move further away we start to see some differences creep in. However, this takes a bit longer for the derived allele than it did for the ancestral allele: the genomic region where all or most of our individuals are colored red is larger than the size of the blue segment in the top half of the figure. This is because the derived allele is newer than the ancestral allele, so there hasn't been as much evolutionary time for new mutation and recombination events to introduce diversity into the subset of individuals that have the red allele. Moreover, this particular allele happens to be positively selected, and it has increased in frequency rather quickly, meaning we have had even less time for mutation and recombination events. We can see that there is a noticeable difference in the total size of the red portion of the bottom half of the plot and the total size of the blue portion in the top half of the plot, and this is a consequence of the rapid spread of the derived allele in this example. The magnitude of the difference between the red portion on the bottom plot and the blue portion on the top plot is in essence the information that _iHS_ is trying to capture. (This isn't exactly what _iHS_ measures, but that is enough of an explanation for now. If you want to see more detail on what exactly _iHS_ calculates, check out the paper: https://doi.org/10.1371/journal.pbio.0040072).\n",
    "\n",
    "## Part 2: Characterizing the behavior of _iHS_ on simulated data\n",
    "\n",
    "Before we start playing around with using _iHS_ to detect selection signatures in real genomic data, we need to be certain that the statistic behaves the way that it should. The best way to do this sort of testing in population genetics is to perform simulations. So we will do just that by using the simulator msprime (https://tskit.dev/msprime/docs/stable/intro.html).\n",
    "\n",
    "### Simulated examples of neutral evolution\n",
    "\n",
    "First we will see what _iHS_ values look like in the absence of natural selection. Let's start by importing some python modules that we will need throughout the exercise, including msprime:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bc695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import msprime\n",
    "import allel\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f896d0",
   "metadata": {},
   "source": [
    "Now that these tools have been imported we can get to work. Let's start by familiarizing ourselves with msprime. First up, we will simulate a 10 kilobase region, with a population of 10000 individuals, recominbation and mutation rates of 1e-8 (similar to rates measured in humans), and we will take a sample of 10 (haploid) individuals:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ab049dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = msprime.sim_ancestry(10, recombination_rate=1e-8, sequence_length=1e4, ploidy=1, population_size=10000)\n",
    "mts = msprime.sim_mutations(ts, rate=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a214c3",
   "metadata": {},
   "source": [
    "That's it! Super easy. But notice how we use two commands here, the first one seems to be simulating the \"ancestry\" of our population sample, and the second one simulates mutations. What does that mean? Well, the first command simply simulates the evolutionary trees connecting the individuals in our sample. I say \"trees\" and not \"tree\" because recombination will cause there to be multiple trees along the sequence (crossover events cause different parts of the genome to have different evolutionary histories). The sim_ancestry function generates our \"tree sequence\" that contains this information.\n",
    "\n",
    "Once we have generated these trees, then we can simply plop mutations down onto different branches the trees according to our specified rate. All descendants of the branch where a mutation occurred will posess that mutation, so it is easy to map these mutations onto individuals in our sample. The sim_mutations function does all of this for us.\n",
    "\n",
    "Once this is done, we are going to want to put this information into a data structure that we can use. Specifically, we are going to be using the scikit-allel module (https://scikit-allel.readthedocs.io/en/stable/) to calculate _iHS_, so we will use allel's HaplotypeArray data structure to store our mutations properly. To do this, we simply need a matrix where the rows correspond to different variants and the columns correspond to each of the 10 individuals in our sample. The code below does all of this, while also printing some stuff out to help you see what is going on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385c21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "hapArray = []\n",
    "posArray = []\n",
    "print(\"mutations occurring during simulation:\")\n",
    "print(\"position\", \"alleles at site\", \"genotypes\", sep=\"\\t\")\n",
    "for var in mts.variants():\n",
    "    hapArray.append(var.genotypes)\n",
    "    posArray.append(int(round(var.site.position)))\n",
    "    print(int(var.site.position), var.alleles, var.genotypes, sep=\"\\t\")\n",
    "print(\"\")\n",
    "\n",
    "# our haplotype array is now a list of lists, so we need to\n",
    "# convert this into allel's HaplotypeArray using the\n",
    "# HaplotypeArray() constructor method\n",
    "hapArray = allel.HaplotypeArray(hapArray)\n",
    "\n",
    "print(\"number of polymorphisms:\", hapArray.n_variants)\n",
    "print(\"number of individuals:\", hapArray.n_haplotypes)\n",
    "print(\"haplotype array:\")\n",
    "print(hapArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743d0dc0",
   "metadata": {},
   "source": [
    "Easy! Now let's see what happens when we simulate a larger region:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c247d1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ts = msprime.sim_ancestry(100, recombination_rate=1e-8, sequence_length=5e6, ploidy=1, population_size=10000)\n",
    "mts = msprime.sim_mutations(ts, rate=1e-8)\n",
    "\n",
    "hapArray = []\n",
    "posArray = []\n",
    "for var in mts.variants():\n",
    "    hapArray.append(var.genotypes)\n",
    "    posArray.append(int(round(var.site.position)))\n",
    "\n",
    "\n",
    "hapArray = allel.HaplotypeArray(hapArray)\n",
    "print(hapArray.n_variants)\n",
    "print(hapArray.n_haplotypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5d96804",
   "metadata": {},
   "source": [
    "This region was 5 megabases in length. But msprime still simulated it in the blink of an eye!\n",
    "\n",
    "Okay, our goal is to calculate _iHS_ scores, so maybe we should get to work on that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eff5bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ihsScores = allel.ihs(hapArray, posArray)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dc868d",
   "metadata": {},
   "source": [
    "Wait, that's all? This scikit-allel package really makes things easy!\n",
    "\n",
    "Well, there is actually a bit of post-processing to do, but it is pretty painless. First, think about what happens when we want to calculate an _iHS_ score near the edge of a chromosome. We can try to look at the red and blue portions of our sample like in the figure above, but we will run over the edge of the chromosome before we get the chance to see any meaningful patterns. In these cases, scikit-allel's ihs function will simply return NaN values (Not a Number), so we might as well remove these from our data before we press on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e1af96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, initialize some empty arrays that we will use to\n",
    "# store the NaN-less versions of all the information we\n",
    "# obtained above: not just the iHS scores, but also the\n",
    "# haplotypes and the positions of polymorphisms in case\n",
    "# we decide we want to use that information later.\n",
    "ihsScoresNanless = []\n",
    "hapArrayNanless = []\n",
    "posArrayNanless = []\n",
    "\n",
    "# iterate through all of our polymorphisms\n",
    "for i in range(len(posArray)):\n",
    "    # grab those who got an iHS score that wasn't a NaN\n",
    "    if not np.isnan(ihsScores[i]):\n",
    "        # populate our ihs, position, and haplotype arrays\n",
    "        # with values corresponding to these non-NaN scores\n",
    "        ihsScoresNanless.append(ihsScores[i])\n",
    "        posArrayNanless.append(posArray[i])\n",
    "        hapArrayNanless.append(hapArray[i])\n",
    "\n",
    "# convert our array of haplotypes into a proper HaplotypeArray\n",
    "hapArrayNanless = allel.HaplotypeArray(hapArrayNanless)\n",
    "\n",
    "# Let's see how many polymorphisms we filtered out\n",
    "print(hapArrayNanless.n_variants)\n",
    "# The number of individuals in our sample is unchanged\n",
    "print(hapArrayNanless.n_haplotypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c58de69",
   "metadata": {},
   "source": [
    "Looks like most of our _iHS_ scores were actually nans! Thsi is because we simulated a fairly small chromosome (5 Mb is pretty tiny in the grand scheme of things). But, let's take a look at the distribution of values we got buy plotting a histogram using the matplotlib module we imported earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d835159",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ihsScoresNanless, bins=20)\n",
    "plt.ylabel('iHS score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207fd5b4",
   "metadata": {},
   "source": [
    "Cool! But notice that our distribution is kind of skewed here. This actually a symptom of a potentially bigger problem: _iHS_ scores will be somewhat dependent on allele frequencies: much rarer, newer alleles will be associated with much less diversity than somewhat older, more common alleles. We can account for this by standardizing our data by alleles frequency. The way this works is that we partition our _iHS_ scores according to the frequency of the derived allele at the target SNP, and then standardize within each bin (i.e. the values within each bin are convered into z-scores: https://en.wikipedia.org/wiki/Standard_score).\n",
    "\n",
    "Let's do this, and see how it affects our histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b5e575",
   "metadata": {},
   "outputs": [],
   "source": [
    "dac = hapArrayNanless.count_alleles()[:,1]\n",
    "ihsScoresStd, stdBins = allel.standardize_by_allele_count(ihsScoresNanless, dac, n_bins=20, diagnostics=False)\n",
    "\n",
    "plt.hist(ihsScoresStd, bins=20)\n",
    "plt.ylabel('iHS score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2938fa4f",
   "metadata": {},
   "source": [
    "This looks a lot better. Maybe still a little noisy beause it is based on a relatively small number of polymorphisms, but we will be looking at much bigger datasets in a bit!\n",
    "\n",
    "### Calculting _iHS_ on simulated selective sweeps\n",
    "\n",
    "That was fun (for me anyway), but our goal is to use _iHS_ to find sweeps. So we really ought to simulate a selective sweep and see what happens. msprime can also do this. We simply have to add the \"model\" information in the following block of code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d67433f",
   "metadata": {},
   "outputs": [],
   "source": [
    "L=1e8 # We are increasing our chromosome length here to compensate for the loss of diversity caused by the sweep.\n",
    "N=10000\n",
    "\n",
    "# define our hard sweep model\n",
    "sweep_model = msprime.SweepGenicSelection(\n",
    "    position=L / 2,  # beneficial mutation location: middle of chrom\n",
    "    start_frequency=1.0 / (2 * N), # starting frequency of the sweeping mutation\n",
    "    end_frequency=0.9, # final frequency of the mutation (incomplete sweep)\n",
    "    s=0.25, # selection coefficient of the beneficial mutation\n",
    "    dt=1e-6, # nevermind this!\n",
    ")\n",
    "\n",
    "ts = msprime.sim_ancestry(\n",
    "    100,\n",
    "    model=[sweep_model, msprime.StandardCoalescent()],\n",
    "    population_size=N,\n",
    "    recombination_rate=1e-8,\n",
    "    sequence_length=L,\n",
    "    ploidy=1,\n",
    ")\n",
    "\n",
    "mts = msprime.sim_mutations(ts, rate=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99e7f3b",
   "metadata": {},
   "source": [
    "So we had to set a few additional parameters related to the sweep: the location of the sweeping mutation, the starting frequency, the frequency at the time of sampling, and the selective advantage (a pretty whopping 25% fitness boost in this case). Then we just ran our simulation as normal. Pretty easy!\n",
    "\n",
    "Now let's process our results in the same manner as for our neutral simulations, and check out the histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d819c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "hapArraySel = []\n",
    "posArraySel = []\n",
    "for var in mts.variants():\n",
    "    hapArraySel.append(var.genotypes)\n",
    "    posArraySel.append(int(round(var.site.position)))\n",
    "\n",
    "\n",
    "hapArraySel = allel.HaplotypeArray(hapArraySel)\n",
    "print(hapArraySel.n_variants)\n",
    "print(hapArraySel.n_haplotypes)\n",
    "ihsScoresSel = allel.ihs(hapArraySel, posArraySel)\n",
    "\n",
    "\n",
    "ihsScoresSelNanless = []\n",
    "hapArraySelNanless = []\n",
    "posArraySelNanless = []\n",
    "for i in range(len(posArraySel)):\n",
    "    if not np.isnan(ihsScoresSel[i]):\n",
    "        ihsScoresSelNanless.append(ihsScoresSel[i])\n",
    "        posArraySelNanless.append(posArraySel[i])\n",
    "        hapArraySelNanless.append(hapArraySel[i])\n",
    "\n",
    "hapArraySelNanless = allel.HaplotypeArray(hapArraySelNanless)\n",
    "\n",
    "print(hapArraySelNanless.n_variants)\n",
    "print(hapArraySelNanless.n_haplotypes)\n",
    "\n",
    "plt.hist(ihsScoresSelNanless, bins=20)\n",
    "plt.ylabel('iHS score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541b51dd",
   "metadata": {},
   "source": [
    "Well that looks different! If you scroll back up and compare to the distribution of our (unstandardized) _iHS_ values from our neutral simulation, you can clearly see that we have a lot more extreme values here. But again, we should standardize by allele frequency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c30d7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "dac = hapArraySelNanless.count_alleles()[:,1]\n",
    "ihsScoresStdSel, stdBinsSel = allel.standardize_by_allele_count(ihsScoresSelNanless, dac, n_bins=20, diagnostics=False)\n",
    "\n",
    "plt.hist(ihsScoresStdSel, bins=20)\n",
    "plt.ylabel('iHS score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17812dbc",
   "metadata": {},
   "source": [
    "That is less crazy, but still it looks like we have an excess of extreme values (fatter tails) compared to our neutral example. Those values were probably calculated at SNPs in the vicinity of the sweep, where we have some \"long range haplotypes\" just like our red example in the figure we looked at earlier. This is probably a sufficent sanity check that our calculations are working as expected. Woohoo! Now let's take a look at...\n",
    "\n",
    "### A more complicated/realistic demographic model\n",
    "\n",
    "One commonly used model of demographic changes over the course of human history is Gutenkunst et al.'s (2009; https://doi.org/10.1371/journal.pgen.1000695) model of the Out-of-Africa migration. This model contains three populations, an African population (abbreviated AFR below), a population of Han-Chinese individuals from Beijing (CHB), and a population of individuals of European ancestry from Utah (abbreviated CEU, with the C itself standing for another abbreviation, CEPH, which stands for Centre dâ€²Etudes du Polymorphisme Humain, which created the dataset that this simulated subpopulation is based on. Ugh.).\n",
    "\n",
    "This model has a lot of moving parts, but thankfully it has all been nicely implemented for us in the msprime documentation, so we can borrow it from there without worrying about making any errors. The code below was taken from https://tskit.dev/msprime/docs/stable/demography.html#sec-demography-examples-population-tree.\n",
    "\n",
    "We looked at this model in the population genetics lectures. Briefly, it begins with an ancestral African population of 7300 individuals. Then, 220,000 years ago (assuming a generation time of 25 years), this population expands to 12,300 individuals---this change is noted to correspond roughly with the emergence of anatomically modern humans, but really this fact has no bearing on the model or the resulting simulations (only genomes are simulated---no phenotypes!). Later, the African population splits into two subpopulations (African and Eurasian) 140,000 years ago, with the initial size of the Eurasian population being 2,100 individuals. Much later, the Eurasian population itself splits into two subpopulations 21.2 thousand years ago, with the initial sizes of the resulting Asian and European populations being 510 and 1,000 individuals, respectively. These two subpopulations then experience expontential growth at rates of 0.4% and 0.55% per year, respectively, until the present day. The final sizes of the Asian and European populations are thus roughly 54,090 and 29,725 individuals, while the African population in this model has remained constant at 12,300 years. The model also includes migration between these subpopulations.\n",
    "\n",
    "The code below creates this demographic model, and the table at the bottom summarizes it. One weird thing that you might notice about the table and code if you look closely is that it seems to be backwards. For example, the initial_size of CEU is 29725, when really that is the size at the present-day. This is because msprime is a coalescent simulator, which means that it simulates these individuals backwards in time until they \"coalesce\" into their most recent common ancestor, giving us a tree (or a tree sequence, when recombination occurs). Kinda weird, but this simulation approach is extremely fast. You can read more about msprime here: https://doi.org/10.1371/journal.pcbi.1004842."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11025ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "# Times are provided in years, so we convert into generations.\n",
    "generation_time = 25\n",
    "T_OOA = 21.2e3 / generation_time\n",
    "T_AMH = 140e3 / generation_time\n",
    "T_ANC = 220e3 / generation_time\n",
    "# We need to work out the starting population sizes based on\n",
    "# the growth rates provided for these two populations\n",
    "r_CEU = 0.004\n",
    "r_CHB = 0.0055\n",
    "N_CEU = 1000 / math.exp(-r_CEU * T_OOA)\n",
    "N_CHB = 510 / math.exp(-r_CHB * T_OOA)\n",
    "N_AFR = 12300\n",
    "\n",
    "demography = msprime.Demography()\n",
    "demography.add_population(\n",
    "    name=\"YRI\",\n",
    "    description=\"Yoruba in Ibadan, Nigeria\",\n",
    "    initial_size=N_AFR,\n",
    ")\n",
    "demography.add_population(\n",
    "    name=\"CEU\",\n",
    "    description=(\n",
    "        \"Utah Residents (CEPH) with Northern and Western European Ancestry\"\n",
    "    ),\n",
    "    initial_size=N_CEU,\n",
    "    growth_rate=r_CEU,\n",
    ")\n",
    "demography.add_population(\n",
    "    name=\"CHB\",\n",
    "    description=\"Han Chinese in Beijing, China\",\n",
    "    initial_size=N_CHB,\n",
    "    growth_rate=r_CHB,\n",
    ")\n",
    "demography.add_population(\n",
    "    name=\"OOA\",\n",
    "    description=\"Bottleneck out-of-Africa population\",\n",
    "    initial_size=2100,\n",
    ")\n",
    "demography.add_population(\n",
    "    name=\"AMH\", description=\"Anatomically modern humans\", initial_size=N_AFR\n",
    ")\n",
    "demography.add_population(\n",
    "    name=\"ANC\",\n",
    "    description=\"Ancestral equilibrium population\",\n",
    "    initial_size=7300,\n",
    ")\n",
    "\n",
    "# Set the migration rates between extant populations\n",
    "demography.set_symmetric_migration_rate([\"CEU\", \"CHB\"], 9.6e-5)\n",
    "demography.set_symmetric_migration_rate([\"YRI\", \"CHB\"], 1.9e-5)\n",
    "demography.set_symmetric_migration_rate([\"YRI\", \"CEU\"], 3e-5)\n",
    "\n",
    "# Join the extant populations into ancestral populations (adjusting migration accordingly)\n",
    "demography.add_population_split(time=T_OOA, derived=[\"CEU\", \"CHB\"], ancestral=\"OOA\")\n",
    "demography.add_symmetric_migration_rate_change(\n",
    "    time=T_OOA, populations=[\"YRI\", \"OOA\"], rate=25e-5\n",
    ")\n",
    "demography.add_population_split(time=T_AMH, derived=[\"YRI\", \"OOA\"], ancestral=\"AMH\")\n",
    "demography.add_population_split(time=T_ANC, derived=[\"AMH\"], ancestral=\"ANC\")\n",
    "\n",
    "demography"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca679ce",
   "metadata": {},
   "source": [
    "Now that we have our demographic model, we are ready to simulate. We are going to sample from the CEU population, with a sample size of 50 (25 diploids), matching the subset of the 1000 Genomes Data that we will be playing with in the final section. Otherwise, these steps will look very familiar. First we run the simulation and make our HaplotypeArray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1d4bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "nCEU = 50\n",
    "ts = msprime.sim_ancestry(\n",
    "    [msprime.SampleSet(nCEU, \"CEU\")],\n",
    "    demography=demography,\n",
    "    sequence_length=5e6,\n",
    "    recombination_rate=1e-8,\n",
    "    ploidy=1)\n",
    "\n",
    "mts = msprime.sim_mutations(ts, rate=1e-8)\n",
    "\n",
    "hapArray = []\n",
    "posArray = []\n",
    "for variant in mts.variants():\n",
    "    hapArray.append(variant.genotypes)\n",
    "    posArray.append(int(round(variant.site.position)))\n",
    "\n",
    "hapArray = allel.HaplotypeArray(hapArray)\n",
    "print(hapArray.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1da25a3",
   "metadata": {},
   "source": [
    "Then we remove our NaNs and plot our histogram:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2605827f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ihsScores = list(allel.ihs(hapArray, posArray))\n",
    "print(len(ihsScores))\n",
    "\n",
    "ihsScoresNanless= []\n",
    "hapArrayNanless = []\n",
    "posArrayNanless = []\n",
    "\n",
    "for i in range(len(posArray)):\n",
    "    if not np.isnan(ihsScores[i]):\n",
    "        ihsScoresNanless.append(ihsScores[i])\n",
    "        posArrayNanless.append(posArray[i])\n",
    "        hapArrayNanless.append(hapArray[i])\n",
    "\n",
    "hapArrayNanless = allel.HaplotypeArray(hapArrayNanless)\n",
    "print(len(ihsScoresNanless))\n",
    "\n",
    "plt.hist(ihsScoresNanless, bins=20)\n",
    "plt.ylabel('iHS score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "052a9dae",
   "metadata": {},
   "source": [
    "This looks a bit different than our original (unstandardized histogram), so maybe the different demographic history had an effect here. Again, we will standardize and plot the histrogra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e668bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "dac = hapArrayNanless.count_alleles()[:,1]\n",
    "ihsScoresStdNanless, stdBinsNanless = allel.standardize_by_allele_count(ihsScoresNanless, dac, n_bins=20, diagnostics=False)\n",
    "\n",
    "plt.hist(ihsScoresStdNanless, bins=20)\n",
    "plt.ylabel('iHS score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20689bc",
   "metadata": {},
   "source": [
    "That looks a lot better. So standardizing will necessarily make the overall distribution fairly robust to demographic changes. The goal is then to look for outliers from this distribution, and see what we can learn about recent adaptation. So let's get our hands dirty and see if we can...\n",
    "\n",
    "## Part 3: Detect signatures selection human population genetic data\n",
    "\n",
    "The time has come to look at some actual genomic data. There should be some in your working directory, specifically, a compressed VCF file called CEU50.chr2LCT.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz. You can read about VCF (variant call format) files here (https://samtools.github.io/hts-specs/VCFv4.2.pdf), and learning this format is essential if you plan on doing population genetic work of any kind in the future, but it is not actually necessary to know to much about it to complete this exercise. That's because scikit-allel can read VCFs for us quite nicely:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53377255",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read vcf\n",
    "callset = allel.read_vcf('CEU50.chr2LCT.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz')\n",
    "print(sorted(callset.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbc1118",
   "metadata": {},
   "source": [
    "There are quite a few entires in this \"callset\" dictionary, but there are only two that we need 'variants/POS', which is basically our positions array (we will finally actually be using this information below), and 'calldata/GT', which is easily convertable to an instance of scikit-allel's GenotypeArray object. GenotypeArrays are different from HaplotypeArrays in that in the former each variant has one two-dimensional entry for each individual, with one dimension for each allele of our diploid individual (GenotypeArrays can also be extended to higher ploidy levels). HaplotypeArrays, on the other hand, have separate entries for each chromosomal copy in each individual. This will hopefully make more sense in a moment after the next two coding steps.\n",
    "\n",
    "If our GenotypeArrays are from 'phased' individuals, then we can readily concert from one to the other and back again. So let's read in our GenotypeArray and convert it to a HaplotypeArray so we can plug it into our ihs function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7483269",
   "metadata": {},
   "outputs": [],
   "source": [
    "posArrayCEU = callset['variants/POS']\n",
    "print(posArrayCEU.shape)\n",
    "\n",
    "gt = allel.GenotypeArray(callset['calldata/GT'])\n",
    "print(gt.shape)\n",
    "\n",
    "hapArrayCEU = gt.to_haplotypes(copy=True)\n",
    "print(hapArrayCEU.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c9eca1",
   "metadata": {},
   "source": [
    "Compare the shapes of the GenotypeArray and HaplotypeArray here and it should make some sense, but let's take a closer look a chunk of 30 SNPs (SNPs 20 through 49, 0-based and inclusive), and our diploid individual at index 12 (the 13th individual in our GenotypeArray), and compare this to our corresponding part of the HaplotpyeArray:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326c8668",
   "metadata": {},
   "outputs": [],
   "source": [
    "print([list(x) for x in gt[20:50,12]])\n",
    "print(hapArrayCEU[20:50,24])\n",
    "print(hapArrayCEU[20:50,25])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802824bd",
   "metadata": {},
   "source": [
    "The first line that we printed out shows the diploid genotypes for individual 12 at each of these 50 SNPs, and 0 and 1 correspond to the reference and alternate alleles (i.e. the allele in the human reference genome, and some other allele), respectively. So we see that this individual is homozygous for the alternate allele at the first of these SNPs for the, and heterozygous at the second to last SNP, with the first haplotpye having the alternate allele, and the second copy having the refernce allele. When we go and look at the corresponding haplotypes from our HaplotypeArray (haplotypes 24 and 25), we see this exact same pattern. So we can see that the phase information that was present in the GenotypeArray was carried over to our HaplotypeArray.\n",
    "\n",
    "Warning: GenotypeArrays can also be used with unphased data, so if you convert those to HaplotypeArrays you will get scrambled haplotypes that don't really mean anything. Be advised!\n",
    "\n",
    "Okay, let's calculate iHS, and plot some histograms. This will again all look very familliar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "098b0dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate iHS scores\n",
    "ihsScoresCEU = list(allel.ihs(hapArrayCEU, posArrayCEU))\n",
    "print(len(ihsScoresCEU))\n",
    "\n",
    "# remove nans\n",
    "ihsScoresCEUNanless = []\n",
    "hapArrayCEUNanless = []\n",
    "posArrayCEUNanless = []\n",
    "\n",
    "for i in range(len(posArrayCEU)):\n",
    "    if not np.isnan(ihsScoresCEU[i]):\n",
    "        ihsScoresCEUNanless.append(ihsScoresCEU[i])\n",
    "        posArrayCEUNanless.append(posArrayCEU[i])\n",
    "        hapArrayCEUNanless.append(hapArrayCEU[i])\n",
    "\n",
    "hapArrayCEUNanless = allel.HaplotypeArray(hapArrayCEUNanless)\n",
    "dacCEU = hapArrayCEUNanless.count_alleles()[:,1]\n",
    "print(len(ihsScoresCEUNanless))\n",
    "\n",
    "plt.hist(ihsScoresCEUNanless, bins=20)\n",
    "plt.ylabel('iHS score')\n",
    "plt.show()\n",
    "\n",
    "ihsScoresCEUStdNanless, stdBins = allel.standardize_by_allele_count(ihsScoresCEUNanless, dacCEU, n_bins=20, diagnostics=False)\n",
    "\n",
    "plt.hist(ihsScoresCEUStdNanless, bins=20)\n",
    "plt.ylabel('iHS score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3889a7b2",
   "metadata": {},
   "source": [
    "It looks like we might have some extreme values here, but to try to make sense of what is going on it would be best to see where these extreme _iHS_ scores are located on the chromosome. We will do this by looking for 50 kilobase windows that have a large fraction of extreme _iHS_ scores. This is motivated by Voight et al.'s finding that in the presence of sweeps, extreme scores tend to be clustered, but that this is not expected under neutrality.\n",
    "\n",
    "So, let's do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b74cc425",
   "metadata": {},
   "outputs": [],
   "source": [
    "wins = {}\n",
    "winSize=50000\n",
    "\n",
    "# loop through all SNPs in our dataset\n",
    "for i in range(len(posArrayCEUNanless)):\n",
    "    pos = posArrayCEUNanless[i]\n",
    "    ihsScore = ihsScoresCEUStdNanless[i]\n",
    "    \n",
    "    # figureout with 50 kb window this SNP belongs to\n",
    "    winIndex = int(pos // winSize)\n",
    "    \n",
    "    # if this window has not been visited yet, initialize it\n",
    "    if not winIndex in wins:\n",
    "        wins[winIndex] = []\n",
    "    \n",
    "    # add the ihs score to the list of values for this window\n",
    "    wins[winIndex].append(abs(ihsScore))\n",
    "\n",
    "winAvgs = {}\n",
    "for winIndex in wins:\n",
    "    \n",
    "    # get all scores in this window with |ihs| > 2 (5% tails on either side)\n",
    "    extremeValsInWin = [1 for x in wins[winIndex] if x > 2]\n",
    "    \n",
    "    # calculate the fraction of all ihs scores in this window that are extreme\n",
    "    winAvgs[winIndex] = len(extremeValsInWin)/len(wins[winIndex])\n",
    "\n",
    "print(\"track type=bedGraph name=ihsOutliers\")\n",
    "for winIndex in sorted(winAvgs):\n",
    "    start = winIndex*winSize\n",
    "    print(\"chr2\", start, start+winSize, winAvgs[winIndex], sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa7e439e",
   "metadata": {},
   "source": [
    "### Browsing our results\n",
    "\n",
    "Okay, how can we visualize this stuff? The simplest way is to go to the UCSC Genome Browser. First, try this link: https://genome.ucsc.edu/cgi-bin/hgTracks?db=hg19&position=chr2%3A134000000-140000000\n",
    "\n",
    "Then, find the button that says \"add custom tracks\" and click it.\n",
    "\n",
    "You will see two text boxes that you can enter data into. You can ignore the bottom one. In the top one (labeled \"Paste URLs or data:\"), copy the large list of results from our previous step--be sure to get everything!\n",
    "\n",
    "Then click submit. and finally, click the UCSC link I shared jsut a few sentences above one last time. Let's chat about what you found!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
